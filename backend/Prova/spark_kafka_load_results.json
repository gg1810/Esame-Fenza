{
    "timestamp": "2026-01-14T08:54:30.751753",
    "config": {
        "num_users": 10000,
        "prob_update": 0.2,
        "prob_delete": 0.1
    },
    "architecture_flow": [
        "1. Script genera 10.000 operazioni realistiche (ADD, UPDATE, DELETE).",
        "2. Scrittura Duale: Script scrive su MongoDB (collection 'movies') E invia evento a Kafka (topic 'user-movie-events').",
        "3. Spark Streaming legge eventi da Kafka come trigger.",
        "4. Spark legge i dati aggiornati da MongoDB 'movies' per l'utente.",
        "5. Spark calcola statistiche complesse (trend, generi, registi preferiti).",
        "6. Spark salva i risultati in MongoDB 'user_stats'."
    ],
    "test_phases": [
        "FASE 1 (Setup): Creazione massiva di 10.000 profili utente nel DB (bulk insert). Non genera traffico Kafka, solo prepara il DB.",
        "FASE 2 (Esecuzione): Il cuore del test. Genera traffico reale simulando utenti che aggiungono, votano o rimuovono film. Misura il throughput di scrittura del sistema.",
        "FASE 3 (Verifica): Monitora la collection 'user_stats' per vedere quanto tempo impiega Spark a processare tutto il backlog generato nella Fase 2."
    ],
    "phase_1_setup": {
        "duration_seconds": 0.13,
        "users_created": 10000
    },
    "phase_2_execution": {
        "duration_seconds": 2.62,
        "total_events": 13094,
        "events_per_second": 5003.73
    },
    "phase_3_verification": {
        "duration_seconds": 5.01,
        "final_stats_count": 8954,
        "target_met": true
    },
    "total_duration_seconds": 7.78
}